import logging
import requests
import subprocess
import sys
import importlib
from transformers import pipeline

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LLMReasoning:
    """
    A class that facilitates interacting with a Large Language Model (LLM) for reasoning tasks.
    Supports both online service-based reasoning and local transformer models.
    """

    def __init__(self, model_url: str = "http://your-llm-service.com/query", local_model_name: str = "bert-large-uncased"):
        """
        Initializes the LLMReasoning instance.
        
        Parameters:
        - model_url (str): The URL of the LLM service. Default is a placeholder.
        - local_model_name (str): The pre-trained model name for local reasoning. Default is 'distilbert-base-uncased'.
        """
        self.model_url = model_url
        self.local_model_name = local_model_name
        self.local_model = None
        self._initialize_local_model()

    def _initialize_local_model(self):
        """
        Tries to initialize the local transformer model for fallback reasoning.
        If the model loading fails, it logs the error.
        """
        try:
            self._ensure_transformers_installed()
            self.local_model = pipeline("question-answering", model=self.local_model_name)
            logger.info(f"Successfully loaded local model: {self.local_model_name}")
        except Exception as e:
            logger.error(f"Local model loading failed: {str(e)}")
            self.local_model = None

    def _ensure_transformers_installed(self):
        """Ensure that the transformers package is installed."""
        try:
            importlib.import_module("transformers")
        except ImportError:
            logger.info("transformers library not found. Installing it now...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "transformers"])

    def query(self, input_text: str, question: str) -> str:
        """
        Queries the LLM to reason over input text and answer the given question.
        
        Parameters:
        - input_text (str): The text input that needs to be reasoned over.
        - question (str): The question to be answered based on the input text.
        
        Returns:
        - str: The response generated by the LLM.
        """
        result = None
        try:
            # First attempt using online LLM service
            result = self._query_online_service(input_text, question)
        except Exception as e:
            logger.warning(f"Online LLM query failed: {e}. Attempting fallback to local model.")
            # Fallback to local model
            if self.local_model:
                result = self._query_local_model(input_text, question)
            else:
                logger.error("No local model available for reasoning.")
                result = "Error: Both online and local models failed."

        return result

    def _query_online_service(self, input_text: str, question: str) -> str:
        """
        Helper function to query the LLM service online.
        
        Parameters:
        - input_text (str): The context text to be used by the model.
        - question (str): The question to be answered.
        
        Returns:
        - str: The response from the online service.
        """
        payload = {"text": input_text, "question": question}
        response = requests.post(self.model_url, json=payload)
        response.raise_for_status()  # Will raise an exception if the status is not OK
        result = response.json().get('answer', 'No answer available')
        logger.info("Received response from online LLM service.")
        return result

    def _query_local_model(self, input_text: str, question: str) -> str:
        """
        Helper function to query the local transformer model for reasoning.
        
        Parameters:
        - input_text (str): The context text to be used by the local model.
        - question (str): The question to be answered.
        
        Returns:
        - str: The answer provided by the local model.
        """
        result = self.local_model(question=question, context=input_text)
        return result['answer']
